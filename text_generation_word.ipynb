{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ranging-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv('GFM_data.csv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "regulation-division",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import string\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unauthorized-preserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.get('Text').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "potential-brain",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def clean_data(txt_elmt):\n",
    "    # txt_elmt is each element (each post) in the \"text\" array\n",
    "    tokens = txt_elmt.split() # each word\n",
    "    \n",
    "    # nothing changes. except removing punctuation\n",
    "    mytable = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [word.translate(mytable) for word in tokens]\n",
    "    return [word.lower() for word in tokens if word.isalpha()]\n",
    "\n",
    "#for i in text:\n",
    "# try with text[0] first\n",
    "tokens = clean_data(text[0]) # list of tokens\n",
    "length = 51 # learn initial 50 words, predict the next word\n",
    "lines = [] # lst of all training sequences\n",
    "sufficiency = 200_000\n",
    "for i in range(len(tokens)):\n",
    "    sequence = tokens[i: i + length] # every 50-word\n",
    "    lines += [' '. join(sequence)]\n",
    "    if i > sufficiency:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "killing-preliminary",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "million-hundred",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token=\"<OOV>\") # OOV adds new words outside the trained words\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines) # all words become numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "facial-ground",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[121 120   4  20 119 118  21 117   9   5 116  19   8 115 114 113 112 111\n",
      "    4  20 110 109  96 108  19  96 107 106   5 105 104 103 102   4  20 101\n",
      "  100   7  18  99   6  17  16   5  98   3   4  97  15  95]]\n",
      "<class 'numpy.ndarray'>\n",
      "[[120   4  20 119 118  21 117   9   5 116  19   8 115 114 113 112 111   4\n",
      "   20 110 109  96 108  19  96 107 106   5 105 104 103 102   4  20 101 100\n",
      "    7  18  99   6  17  16   5  98   3   4  97  15  95  22]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([121., 120.,   4.,  20., 119., 118.,  21., 117.,   9.,   5., 116.,\n",
       "        19.,   8., 115., 114., 113., 112., 111.,   4.,  20., 110., 109.,\n",
       "        96., 108.,  19.,  96., 107., 106.,   5., 105., 104., 103., 102.,\n",
       "         4.,  20., 101., 100.,   7.,  18.,  99.,   6.,  17.,  16.,   5.,\n",
       "        98.,   3.,   4.,  97.,  15.,  95., 120.,   4.,  20., 119., 118.,\n",
       "        21., 117.,   9.,   5., 116.,  19.,   8., 115., 114., 113., 112.,\n",
       "       111.,   4.,  20., 110., 109.,  96., 108.,  19.,  96., 107., 106.,\n",
       "         5., 105., 104., 103., 102.,   4.,  20., 101., 100.,   7.,  18.,\n",
       "        99.,   6.,  17.,  16.,   5.,  98.,   3.,   4.,  97.,  15.,  95.,\n",
       "        22.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training and test sets\n",
    "X = np.array([])\n",
    "y = np.array([])\n",
    "for i in sequences[:2]:\n",
    "    #X += [i[:-1]]\n",
    "    #y += [i[-1]]\n",
    "    i = np.array([i[:-1]])\n",
    "    print(type(i))\n",
    "    print(i)\n",
    "    X = np.append(X, i)\n",
    "#X = np.array(X, dtype=object)\n",
    "#y = np.array(y)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bulgarian-escape",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "sequence_length = X[0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "focal-effort",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length = sequence_length)) # input dim - vocabsize, output dim = 50\n",
    "# 100 hidden layers\n",
    "model.add(LSTM(100, return_sequences=True)) # do twice, return_sequences=True\n",
    "model.add(LSTM(100)) \n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax')) # we predict each word ==> use size = vocab_size, activation = softmax to get the prob for each predicted word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "subtle-family",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 1, 50)             6100      \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 1, 100)            60400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 122)               12322     \n",
      "=================================================================\n",
      "Total params: 169,322\n",
      "Trainable params: 169,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sporting-dominant",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "automotive-museum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 122), dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = np.asarray(y, dtype=object)\n",
    "train_y = train_y.astype(np.float32)\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "rotary-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.asarray([X], dtype=object)[0]\n",
    "train_X = train_X.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_X, train_y, batch_size=256, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "supreme-auction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([121., 120.,   4.,  20., 119., 118.,  21., 117.,   9.,   5., 116.,\n",
       "        19.,   8., 115., 114., 113., 112., 111.,   4.,  20., 110., 109.,\n",
       "        96., 108.,  19.,  96., 107., 106.,   5., 105., 104., 103., 102.,\n",
       "         4.,  20., 101., 100.,   7.,  18.,  99.,   6.,  17.,  16.,   5.,\n",
       "        98.,   3.,   4.,  97.,  15.,  95., 120.,   4.,  20., 119., 118.,\n",
       "        21., 117.,   9.,   5., 116.,  19.,   8., 115., 114., 113., 112.,\n",
       "       111.,   4.,  20., 110., 109.,  96., 108.,  19.,  96., 107., 106.,\n",
       "         5., 105., 104., 103., 102.,   4.,  20., 101., 100.,   7.,  18.,\n",
       "        99.,   6.,  17.,  16.,   5.,  98.,   3.,   4.,  97.,  15.,  95.,\n",
       "        22.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = np.asarray([X], dtype=object)[0]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-spice",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
